{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression using Gradiant Decent algorithm\n",
    "\n",
    "In linear regression there are $n$ input features and $1$ output feature that we want to predict. \n",
    "So visually what we're trying to do, is that we want to find a line \\_n-dimensional\\_ that \"fits\" data points. or more presicely, is as close as possible to all data points.\n",
    "\n",
    "The description above gives us an idea about what we're looking for but it can't be considered as a problem statement.\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 What is the problem?\n",
    "Making a clear problem statement is as important as solving the problem.\n",
    "for that matter, let's note that we can model our input features with a vector $x^{(i)}$ in a vector space $V$ and output is a scaler $y^{(i)}$ in the field $F$ in which we define the vector space. here $V = \\mathbb{R}^n$ and $F = \\mathbb{R}$ and superscript $(i)$ indicates sample index.\n",
    "\n",
    "\n",
    "so we can form a matrix $X$ which contains all of sample vectors ($x^{(i)}$):\n",
    "$$\n",
    "X_{ij} = x^{(i)}_j\n",
    "$$\n",
    "\n",
    "Now, let's assume that there is a function $h^*: V \\rightarrow F$ that maps these points to a scaler and is the function that exactly \"fits\" all datapoints. So $h^*(x^{(i)})$. Obviously it's not neccesserily linear or any other form.\n",
    "\n",
    "We define $h$ as the *hypothesis* \\_an estimation of $h^*$\\_ given the constraints that $h$ is a linear function.\n",
    "\n",
    "As it's known that every linear function can be represented with a vector of coefficients, the problem of finding $h$ is equivalent to finding it's vector of coefficients,\n",
    "which is represented by $\\Theta = [\\theta_{1},\\dots, \\theta_{n}]$ . So it's convinient to write $h_{\\Theta}$ instead of just $h$.\n",
    "\n",
    "now we're ready to write the problem statement.\n",
    "\n",
    "#### statement 1:\n",
    "> Given value of $h^*$ for m points/vectors $x^{(1)}, \\dots,x^{(m)}$,  find a linear function $h_{\\Theta}$ which estimates $h^*$.\n",
    "\n",
    "From the terms \"as close as possible\" (1.) and \"estimates\" (1.2) it's not very clear what we should do. In order to define a better metric for that, which means how good some function $h_{\\Theta}$ is we define a cost function $J : V \\rightarrow F$ as:\n",
    "\n",
    "\n",
    "Its convenient to summorize $h_\\Theta(x^{(i)})$ as $\\hat{y}^{(i)}$, so:\n",
    "$$ J(\\Theta) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y}^{(i)}-y^{(i)})^2 $$\n",
    "So we want to find some $h_{\\Theta}$ which minimizes the value of $J$.\n",
    "\n",
    "now we can update the problem statement as:\n",
    "\n",
    "#### statement 2:\n",
    "> Given value of $h^*$ for m points/vectors $x^{(1)}, \\dots,x^{(m)}$,  find a linear function $h_{\\Theta}$ for which $J(\\Theta)$ is minimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(self, x):\n",
    "    res = 0\n",
    "    # add fake feature\n",
    "    x.append(1)\n",
    "    for i in range(self.n):\n",
    "        res += x[i]*self.theta[i]\n",
    "    return res\n",
    "\n",
    "def J(self):\n",
    "    res = 0\n",
    "    for i in range(self.m):\n",
    "        res += (self.h(self.X[i])-self.y[i])**2\n",
    "    res /= 2*self.m\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Solution\n",
    "We use gradiant decent algorithm to find some $\\Theta$ which is local optimum for $J$.\n",
    "\n",
    "This is an overview on how this algorithm works:\n",
    "\n",
    "Let's assume we have $\\Theta_{1}$ as first hypothesis. we can initialize this to some random vector.\n",
    "\n",
    "We claim that $J(\\Theta_{2}) \\leq J(\\Theta_{1})$ for $\\Theta_{2} = \\Theta_{1} - \\eta \\nabla J(\\Theta_{1})$.\n",
    "\n",
    "Doing this $p-1$ times we'll end up with a seqence $\\Theta_{1}, \\dots, \\Theta_{p}$ and each one is a better estimation than the previous one.\n",
    "\n",
    "\n",
    "Next we calculate gradiant of cost funcion this way:\n",
    "$$\n",
    "\\nabla J(\\Theta) = [\\frac{\\partial}{\\partial \\theta_{1}} J, \\dots,  \\frac{\\partial}{\\partial \\theta_{n}} J]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_{j}} J = \\frac{1}{m} \\sum_{i=1}^m(\\hat{y}^{(i)} - y^{i})x^{(i)}_j\n",
    "$$\n",
    "\n",
    "we can calculate sum of these terms and show that:\n",
    "$$\\nabla J = Y^T X$$\n",
    "where $ Y = \\frac{1}{m}[\\hat{y}^{(1)} - y^{(1)}, \\dots, \\hat{y}^{(m)}- y^{(m)}] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiant(self):\n",
    "    gradiant_vector = [0]*self.n\n",
    "    for j in range(self.n):\n",
    "        for i in range(self.m):\n",
    "            gradiant_vector[j] += (self.h(self.X[i])-self.y[i])*self.X[i][j]\n",
    "        gradiant_vector[j] /= self.m\n",
    "    return gradiant_vector\n",
    "    \n",
    "def gradiant_decent(self):\n",
    "    for i in range(self.p):\n",
    "        for j in range(self.n):\n",
    "            self.theta[j] -= self.nabla*self.gradiant()[j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Wrap up\n",
    "This is everything put together in Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, number_of_iterations, learning_rate):\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def h(self, x):\n",
    "        return self.theta.dot(x)\n",
    "\n",
    "    def J(self):\n",
    "        res = 0\n",
    "        for i in range(self.n):\n",
    "            res += (self.h(self.X[i])-self.Y[i])**2\n",
    "        res /= 2*self.n\n",
    "        return res\n",
    "\n",
    "    def gradiant(self):\n",
    "        nabla = np.array(self.n)\n",
    "        Y = np.zeros(self.m)\n",
    "        for i in range(self.m):\n",
    "            Y[i] = (self.h(self.X[i])-self.y[i])/self.m\n",
    "        nabla = Y.dot(self.X)\n",
    "        if (np.linalg.norm(nabla) != 0):\n",
    "            nabla /= np.linalg.norm(nabla)\n",
    "        return nabla\n",
    "        \n",
    "    def gradiant_decent(self):\n",
    "        for k in range(self.number_of_iterations):\n",
    "            nabla = self.gradiant()\n",
    "            for j in range(self.n):\n",
    "                self.theta[j] -= self.learning_rate*nabla[j]\n",
    "            \n",
    "    def run(self, input_dataset, output_dataset):\n",
    "        self.X = np.array(input_dataset)\n",
    "        self.y = np.array(output_dataset)\n",
    "        \n",
    "        # m(number of data), n(number of features or dimentions)\n",
    "        self.m = self.X.shape[0]\n",
    "        self.n = self.X.shape[1]\n",
    "        self.theta = np.ones(self.n)\n",
    "        print(self.n, self.m)\n",
    "\n",
    "        self.gradiant_decent()\n",
    "\n",
    "    def test(self, X_test, Y_test):\n",
    "        X_test = np.array(X_test)\n",
    "        y_hat = np.zeros(len(X_test))\n",
    "        for i in range(len(X_test)):\n",
    "            y_hat[i] = self.h(X_test[i])\n",
    "            \n",
    "        print(self.theta)\n",
    "        print(y_hat)\n",
    "        print(mean_absolute_error(y_hat, Y_test))\n",
    "        print((mean_squared_error(y_hat, Y_test))**0.5)\n",
    "        print(r2_score(y_hat, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Testing with dataset\n",
    "Next, we use a dataset to test everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"~/downloads/Flight_Price_Dataset_Q2.csv\")\n",
    "# print(df.head())\n",
    "departure_time_mapping = {\n",
    "    \"Early_Morning\": 4,\n",
    "    \"Morning\": 3,\n",
    "    \"Afternoon\": 2,\n",
    "    \"Night\": 1, \n",
    "    \"Late_Night\": 0\n",
    "}\n",
    "stops_mapping = {\n",
    "    \"zero\": 2,\n",
    "    \"one\": 1,\n",
    "    \"two_or_more\": 0\n",
    "}\n",
    "class_mapping = {\n",
    "    \"Economy\": 0,\n",
    "    \"Business\": 1\n",
    "}\n",
    "df[\"departure_time\"] = df[\"departure_time\"].map(departure_time_mapping)\n",
    "df[\"stops\"] = df[\"stops\"].map(stops_mapping)\n",
    "df[\"arrival_time\"] = df[\"arrival_time\"].map(departure_time_mapping)\n",
    "df[\"class\"] = df[\"class\"].map(class_mapping)\n",
    "df[\"duration\"] = (50 - df[\"duration\"]) / 10\n",
    "df[\"days_left\"] = df[\"days_left\"] / 10\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "Y = df[\"price\"]\n",
    "X = df.drop(\"price\", axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_test[\"fake_feature\"] = 1\n",
    "X_train[\"fake_feature\"] = 1\n",
    "# print(X_train.head())\n",
    "# print(X_train.value_counts('duration'))\n",
    "# print(X_train['duration'].min(), X_train['duration'].max())\n",
    "# print(X_train.value_counts('days_left'))\n",
    "# print(X_train['days_left'].min(), X_train['days_left'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 122194\n",
      "Training Time: 147.369536 seconds\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "number_of_iterations = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "# reg = Regression(number_of_iterations, learning_rate)\n",
    "# x = [\n",
    "#     [4, 1],\n",
    "#     [9, 3],\n",
    "#     [19, 8],\n",
    "# ]\n",
    "# y = [2, 6, 16]\n",
    "# reg.run(x, y)\n",
    "\n",
    "reg.run(X_train, Y_train)\n",
    "print(\"Training Time: %s seconds\" % round(time.time() - start_time, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.54759561 2.8325589  4.16962846 2.37576285 7.4569713  5.3826233\n",
      " 2.76690916]\n",
      "[81.73483421 72.67262773 54.45878435 ... 60.34540564 66.81885484\n",
      " 78.78713066]\n",
      "19967.78459402524\n",
      "29877.497401459783\n",
      "-6058599.025501116\n"
     ]
    }
   ],
   "source": [
    "# xt = [\n",
    "#     [9, 4],\n",
    "#     [2, 2],\n",
    "#     [13, 5],\n",
    "# ]\n",
    "# yt = [8, 4, 10]\n",
    "# reg.test(xt, yt)\n",
    "reg.test(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4624.881513241347\n",
      "7148.265547735831\n",
      "0.896542234050758\n"
     ]
    }
   ],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(X_train.values.tolist(), Y_train.values.tolist())\n",
    "Y_pred = regr.predict(X_test.values.tolist())\n",
    "print(mean_absolute_error(Y_test.values.tolist(), Y_pred))\n",
    "print((mean_squared_error(Y_test.values.tolist(), Y_pred))**0.5)\n",
    "print(r2_score(Y_test.values.tolist(), Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
