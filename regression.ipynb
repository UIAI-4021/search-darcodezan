{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression using Gradiant Decent algorithm\n",
    "\n",
    "In linear regression there are $n$ input features and $1$ output feature that we want to predict. \n",
    "So visually what we're trying to do, is that we want to find a line \\_n-dimensional\\_ that \"fits\" data points. or more presicely, is as close as possible to all data points.\n",
    "\n",
    "The description above gives us an idea about what we're looking for but it can't be considered as a problem statement.\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 What is the problem?\n",
    "Making a clear problem statement is as important as solving the problem.\n",
    "for that matter, let's note that we can model our input features with a vector $x^{(i)}$ in a vector space $V$ and output is a scaler $y^{(i)}$ in the field $F$ in which we define the vector space. here $V = \\mathbb{R}^n$ and $F = \\mathbb{R}$ and superscript $(i)$ indicates sample index.\n",
    "\n",
    "\n",
    "so we can form a matrix $X$ which contains all of sample vectors ($x^{(i)}$):\n",
    "$$\n",
    "X_{ij} = x^{(i)}_j\n",
    "$$\n",
    "\n",
    "Now, let's assume that there is a function $h^*: V \\rightarrow F$ that maps these points to a scaler and is the function that exactly \"fits\" all datapoints. So $h^*(x^{(i)})$. Obviously it's not neccesserily linear or any other form.\n",
    "\n",
    "We define $h$ as the *hypothesis* \\_an estimation of $h^*$\\_ given the constraints that $h$ is a linear function.\n",
    "\n",
    "As it's known that every linear function can be represented with a vector of coefficients, the problem of finding $h$ is equivalent to finding it's vector of coefficients,\n",
    "which is represented by $\\Theta = [\\theta_{1},\\dots, \\theta_{n}]$ . So it's convinient to write $h_{\\Theta}$ instead of just $h$.\n",
    "\n",
    "now we're ready to write the problem statement.\n",
    "\n",
    "#### statement 1:\n",
    "> Given value of $h^*$ for m points/vectors $x^{(1)}, \\dots,x^{(m)}$,  find a linear function $h_{\\Theta}$ which estimates $h^*$.\n",
    "\n",
    "From the terms \"as close as possible\" (1.) and \"estimates\" (1.2) it's not very clear what we should do. In order to define a better metric for that, which means how good some function $h_{\\Theta}$ is we define a cost function $J : V \\rightarrow F$ as:\n",
    "\n",
    "\n",
    "Its convenient to summorize $h_\\Theta(x^{(i)})$ as $\\hat{y}^{(i)}$, so:\n",
    "$$ J(\\Theta) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y}^{(i)}-y^{(i)})^2 $$\n",
    "So we want to find some $h_{\\Theta}$ which minimizes the value of $J$.\n",
    "\n",
    "now we can update the problem statement as:\n",
    "\n",
    "#### statement 2:\n",
    "> Given value of $h^*$ for m points/vectors $x^{(1)}, \\dots,x^{(m)}$,  find a linear function $h_{\\Theta}$ for which $J(\\Theta)$ is minimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of functions described above\n",
    "\n",
    "def h(self, x):\n",
    "    return self.theta.dot(x)\n",
    "\n",
    "def J(self):\n",
    "    res = 0\n",
    "    for i in range(self.n):\n",
    "        res += (self.h(self.X[i])-self.y[i])**2\n",
    "    res /= 2*self.n\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Solution\n",
    "We use gradiant decent algorithm to find some $\\Theta$ which is local optimum for $J$.\n",
    "\n",
    "This is an overview on how this algorithm works:\n",
    "\n",
    "Let's assume we have $\\Theta_{1}$ as first hypothesis. we can initialize this to some random vector.\n",
    "\n",
    "We claim that $J(\\Theta_{2}) \\leq J(\\Theta_{1})$ for $\\Theta_{2} = \\Theta_{1} - \\eta \\nabla J(\\Theta_{1})$.\n",
    "\n",
    "Doing this $p-1$ times we'll end up with a seqence $\\Theta_{1}, \\dots, \\Theta_{p}$ and each one is a better estimation than the previous one.\n",
    "\n",
    "\n",
    "Next we calculate gradiant of cost funcion this way:\n",
    "$$\n",
    "\\nabla J_{\\Theta} = [\\frac{\\partial}{\\partial \\theta_{1}} J, \\dots,  \\frac{\\partial}{\\partial \\theta_{n}} J]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_{j}} J = \\frac{1}{m} \\sum_{i=1}^m(\\hat{y}^{(i)} - y^{i})x^{(i)}_j\n",
    "$$\n",
    "\n",
    "we can calculate sum of these terms and show that:\n",
    "$$\\nabla J = Y^T X$$\n",
    "where $ Y = \\frac{1}{m}[\\hat{y}^{(1)} - y^{(1)}, \\dots, \\hat{y}^{(m)}- y^{(m)}] $\n",
    "\n",
    "There are also some considerations added to the code below, for example we normalize $\\nabla J_{\\Theta}$ in order for $\\eta$ _(learning rate)_ to have the same effect for $p$ times of gradiant computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of gradient decent algorithm\n",
    "\n",
    "def gradiant(self):\n",
    "    nabla = np.array(self.n)\n",
    "    Y = np.zeros(self.m)\n",
    "    for i in range(self.m):\n",
    "        Y[i] = (self.h(self.X[i])-self.y[i])/self.m\n",
    "    nabla = Y.dot(self.X)\n",
    "    if (np.linalg.norm(nabla) != 0):\n",
    "        nabla /= np.linalg.norm(nabla)\n",
    "    return nabla\n",
    "        \n",
    "def gradiant_decent(self):\n",
    "    for k in range(self.number_of_iterations):\n",
    "        nabla = self.gradiant()\n",
    "        for j in range(self.n):\n",
    "            self.theta[j] -= self.learning_rate*nabla[j]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Wrap up\n",
    "This is everything put together in Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression class\n",
    "class Regression:\n",
    "    \n",
    "    def __init__(self, number_of_iterations, learning_rate):\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def h(self, x):\n",
    "        return self.theta.dot(x)\n",
    "\n",
    "    def J(self):\n",
    "        res = 0\n",
    "        for i in range(self.n):\n",
    "            res += (self.h(self.X[i])-self.y[i])**2\n",
    "        res /= 2*self.n\n",
    "        return res\n",
    "\n",
    "    def gradiant(self):\n",
    "        nabla = np.array(self.n)\n",
    "        Y = np.zeros(self.m)\n",
    "        for i in range(self.m):\n",
    "            Y[i] = (self.h(self.X[i])-self.y[i])/self.m\n",
    "        nabla = Y.dot(self.X)\n",
    "        if (np.linalg.norm(nabla) != 0):\n",
    "            nabla /= np.linalg.norm(nabla)\n",
    "        return nabla\n",
    "        \n",
    "    def gradiant_decent(self):\n",
    "        for k in range(self.number_of_iterations):\n",
    "            nabla = self.gradiant()\n",
    "            for j in range(self.n):\n",
    "                self.theta[j] -= self.learning_rate*nabla[j]\n",
    "            \n",
    "    def run(self, input_dataset, output_dataset):\n",
    "        self.X = np.array(input_dataset)\n",
    "        self.y = np.array(output_dataset)\n",
    "        \n",
    "        # m(number of data), n(number of features or dimentions)\n",
    "        self.m = self.X.shape[0]\n",
    "        self.n = self.X.shape[1]\n",
    "        self.theta = np.zeros(self.n)\n",
    "        self.gradiant_decent()\n",
    "\n",
    "    def test(self, X_test, Y_test):\n",
    "        X_test = np.array(X_test)\n",
    "        y_hat = np.zeros(len(X_test))\n",
    "        for i in range(len(X_test)):\n",
    "            y_hat[i] = self.h(X_test[i])\n",
    "            \n",
    "        print(\"Theta:\", self.theta)\n",
    "        print(\"MAE:\", mean_absolute_error(y_hat, Y_test))\n",
    "        print(\"MSE\", (mean_squared_error(y_hat, Y_test))**0.5)\n",
    "        print(\"R2Score:\", r2_score(y_hat, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Testing with dataset\n",
    "Next, we use a dataset to test out implementation of algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv file\n",
    "df = pd.read_csv(\"~/downloads/Flight_Price_Dataset_Q2.csv\")\n",
    "\n",
    "# some mappings for non-numeric data\n",
    "departure_time_mapping = {\n",
    "    \"Early_Morning\": 1,\n",
    "    \"Morning\": 3,\n",
    "    \"Afternoon\": 4,\n",
    "    \"Night\": 2, \n",
    "    \"Late_Night\": 0\n",
    "}\n",
    "stops_mapping = {\n",
    "    \"zero\": 2,\n",
    "    \"one\": 1,\n",
    "    \"two_or_more\": 0\n",
    "}\n",
    "class_mapping = {\n",
    "    \"Economy\": 0,\n",
    "    \"Business\": 1\n",
    "}\n",
    "df[\"departure_time\"] = df[\"departure_time\"].map(departure_time_mapping)\n",
    "df[\"stops\"] = df[\"stops\"].map(stops_mapping)\n",
    "df[\"arrival_time\"] = df[\"arrival_time\"].map(departure_time_mapping)\n",
    "df[\"class\"] = df[\"class\"].map(class_mapping)\n",
    "\n",
    "# remove nan data and normalize it\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "sclr = MinMaxScaler()\n",
    "df_norm = pd.DataFrame(sclr.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# split data to test and train\n",
    "Y = df_norm[\"price\"]\n",
    "X = df_norm.drop(\"price\", axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# add fake feature for regression\n",
    "X_test[\"fake_feature\"] = 1\n",
    "X_train[\"fake_feature\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 58.986542 seconds\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "number_of_iterations = 500\n",
    "\n",
    "start_time = time.time()\n",
    "reg = Regression(number_of_iterations, learning_rate)\n",
    "reg.run(X_train, Y_train)\n",
    "print(\"Training Time: %s seconds\" % round(time.time() - start_time, 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [-0.00114002 -0.10574567 -0.01078645  0.36853972  0.0105679  -0.04968311\n",
      "  0.12565459]\n",
      "MAE: 0.03850491963723872\n",
      "MSE 0.05986853893669113\n",
      "R2Score: 0.8795612906037582\n"
     ]
    }
   ],
   "source": [
    "# results of our regression model\n",
    "reg.test(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Resource and References:\n",
    "[Numpy](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)\n",
    "\n",
    "[Python](https://www.python.org/)\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/)\n",
    "\n",
    "[sklearn](https://scikit-learn.org/stable/)\n",
    "\n",
    "[Stanford CS229: Machine Learning - Linear Regression and Gradient Descent | Lecture 2 (Autumn 2018)](https://youtu.be/4b4MUYve_U8?si=ewEth2LYdMXtPqmN)\n",
    "\n",
    "[DigitalOcean](https://www.digitalocean.com/community/tutorials/normalize-data-in-python)\n",
    "\n",
    "### 1.6 Contributes\n",
    "Pouya Rahimpour 4003613031\n",
    "\n",
    "Mohamad Mazrouei 4003613056\n",
    "\n",
    "MohamadHosein Chahkandi 4003613019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
